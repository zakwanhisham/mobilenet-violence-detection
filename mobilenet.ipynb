{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Detection using mobilenet\n",
    "\n",
    "This project use mobilenet as the core model.\n",
    "\n",
    "What you need to do to run this is to run the:\n",
    "1. import cell\n",
    "2. Video_Play function\n",
    "3. Constant cell\n",
    "4. save_model() and load_model()\n",
    "5. load_model(\"model\")\n",
    "6. predict_frame()\n",
    "7. predict_webcam()\n",
    "8. Inference cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the entry point for the package. It's called by the installer and runs the packages\n",
    "%%bash\n",
    "pip install tensorflow\n",
    "pip install keras\n",
    "pip install opencv-python\n",
    "pip install scikit-learn\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-23 15:55:50.331332: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-23 15:55:50.517750: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-23 15:55:50.517849: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-23 15:55:50.547633: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-23 15:55:50.617093: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-23 15:55:50.618513: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-23 15:55:51.800644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow\n",
    "import keras\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use(\"seaborn\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Video_Play(filepath):\n",
    "    \"\"\"\n",
    "     Plays a video file and waits for user input. This is a blocking function so it should be called from a thread\n",
    "     \n",
    "     Args:\n",
    "     \t filepath: Path to the video\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "\n",
    "    # Open video file if the video file is open\n",
    "    if (cap.isOpened == False):\n",
    "        print(\"Error opening video file\")\n",
    "    \n",
    "    # Read the current cap from the cap stream and display it.\n",
    "    while (cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        # if ret is True cv2. imshow frame frame\n",
    "        if ret == True:\n",
    "            cv2.imshow('Frame', frame)\n",
    "\n",
    "            # Wait for a key to be pressed.\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and list all the video files present in the Classes Directory. Randomly select a video file\n",
    "# Classes Directories\n",
    "NonViolnceVideos_Dir = \"real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/\"\n",
    "ViolnceVideos_Dir = \"real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/\"\n",
    "\n",
    "# Retrieve the list of all the video files present in the Class Directory.\n",
    "NonViolence_files_names_list = os.listdir(NonViolnceVideos_Dir)\n",
    "Violence_files_names_list = os.listdir(ViolnceVideos_Dir)\n",
    "\n",
    "# Randomly select a video file from the Classes Directory.\n",
    "Random_NonViolence_Video = random.choice(NonViolence_files_names_list)\n",
    "Random_Violence_Video = random.choice(Violence_files_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video_Play(f\"{NonViolnceVideos_Dir}/{Random_NonViolence_Video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant\n",
    "\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64,64\n",
    "SEQUENCE_LENGTH = 16\n",
    "\n",
    "DATASET_DIR = \"real-life-violence-situations-dataset/Real Life Violence Dataset/\"\n",
    "CLASSES_LIST = [\"NonViolence\", \"Violence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    \"\"\"\n",
    "    Extracts frames from Video File and returns them as a list. The list is sorted by frame number and the frames are normalized to the width and height\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the Video File\n",
    "    \n",
    "    Returns: \n",
    "        List of frames in the Video File ( Image format ) as a list of NumPy arrays ( grayscale\n",
    "    \"\"\"\n",
    "\n",
    "    frames_list = []\n",
    "\n",
    "    # Read the Video File\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    # Iterate through the Video Frames.\n",
    "    # Reads the next frame from the video.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "\n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "\n",
    "        # Reading the frame from the video.\n",
    "        success, frame = video_reader.read()\n",
    "\n",
    "        # If success is true break the loop until the next call to this function is successful.\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Resize the Frame to fixed height and width.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        # Normalize the resized frame\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        # Append the normalized frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    "\n",
    "\n",
    "    video_reader.release()\n",
    "\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Creates and returns a dataset of repectivities. This is a function that takes as input a list of video files and extracts the frames of each video file.\n",
    "    \n",
    "    \n",
    "    Returns: \n",
    "        A list of features and a list of labels for each video file that was extracted and the path to the video\n",
    "    \"\"\"\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "\n",
    "    # Iterating through all the classes.\n",
    "    # Extract the data of the data of a specific class.\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "\n",
    "        # Get the list of video files present in the specific class name directory.\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "\n",
    "        # Iterate through all the files present in the files list.\n",
    "        # Extract the frames of the video files from the files_list and add them to the features and labels.\n",
    "        for file_name in files_list:\n",
    "\n",
    "            # Get the complete video path.\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    "\n",
    "            # Extract the frames of the video file.\n",
    "            frames = frames_extraction(video_file_path)\n",
    "\n",
    "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified.\n",
    "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
    "            # Append the data to the features labels labels and video files paths.\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "\n",
    "                # Append the data to their repective lists.\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "\n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates and saves a dataset. This is a convenience function for use with scipy. ndimage. catalyst_util\n",
    "features, labels, video_file_paths = create_dataset()\n",
    "\n",
    "np.save(\"features.npy\", features)\n",
    "np.save(\"labels.npy\", labels)\n",
    "np.save(\"video_files_paths.npy\", video_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from disk and return features labels and video_files_paths. This is called by a function that is in charge of loading the data\n",
    "features, labels, video_files_paths = np.load(\"features.npy\") , np.load(\"labels.npy\") , np.load(\"video_files_paths.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels into one-hot-encoded vectors\n",
    "one_hot_encoded_labels = to_categorical(labels)\n",
    "\n",
    "# Split the Data into Train ( 90% ) and Test Set ( 10% ).\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1,\n",
    "                                                                            shuffle = True, random_state = 42)\n",
    "\n",
    "print(features_train.shape,labels_train.shape )\n",
    "print(features_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hack to make it easier to use Mobilenet's built - in version\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "mobilenet = MobileNetV2( include_top=False , weights=\"imagenet\")\n",
    "\n",
    "#Fine-Tuning to make the last 40 layer trainable\n",
    "mobilenet.trainable=True\n",
    "\n",
    "for layer in mobilenet.layers[:-40]:\n",
    "  layer.trainable=False\n",
    "\n",
    "mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Creates and returns a Sequential model that can be used to train the model. This is the first step in the training process.\n",
    "    \n",
    "    \n",
    "    Returns: \n",
    "        A : class : ` neural. Model ` instance that can be used to train the model. Note that the model is created in the following way\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "    #Specifying Input to match features shape\n",
    "    model.add(Input(shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "\n",
    "    # Passing mobilenet in the TimeDistributed layer to handle the sequence\n",
    "    model.add(TimeDistributed(mobilenet))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    lstm_fw = LSTM(units=32)\n",
    "    lstm_bw = LSTM(units=32, go_backwards = True)\n",
    "\n",
    "    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
    "\n",
    "    ########################################################################################################################\n",
    "    early_stopping_callback = EarlyStopping(monitor= 'val_accuracy', patience = 10, restore_best_weights=True)\n",
    "    reduce_lr = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=5,\n",
    "                                                             min_lr=0.00005, verbose=1)\n",
    "\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(x=features_train, y=labels_train, epochs = 50, batch_size=8, shuffle=True, validation_split=0.2, callbacks=[early_stopping_callback, reduce_lr])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    \"\"\"\n",
    "     Save model to disk. This will save the model's JSON and HDF5 file as well as the weights.\n",
    "     \n",
    "     Args:\n",
    "     \t model: The model to save. Must have a : py : class : ` ~gensim. models. BaseNeuralModel ` class.\n",
    "     \t model_name: The name of the model to save\n",
    "    \"\"\"\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_name + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"\n",
    "     Loads a Keras model from disk. This is a convenience function for loading a Keras model from disk.\n",
    "     \n",
    "     Args:\n",
    "     \t model_name: The name of the model to load.\n",
    "     \n",
    "     Returns: \n",
    "     \t A Keras model that was loaded from disk or None if the model couldn't be loaded ( in which case an error is logged\n",
    "    \"\"\"\n",
    "    with open(model_name + \".json\", \"r\") as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Model\n",
    "MoBiLSTM_model = create_model()\n",
    "\n",
    "# Plot the structure of the contructed LRCN model.\n",
    "# plot_model(MoBiLSTM_model, to_file = 'MobBiLSTM_model_structure_plot.png', show_shapes = True, show_layer_names = True)\n",
    "\n",
    "save_model(MoBiLSTM_model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "MoBiLSTM_model = load_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function computes the metagenomic consensus matrix for the predicting features. In this case we have a confusion matrix that is based on the accuracy_score function\n",
    "labels_predict = MoBiLSTM_model.predict(features_test)\n",
    "\n",
    "# Decoding the data to use in Metrics\n",
    "labels_predict = np.argmax(labels_predict , axis=1)\n",
    "labels_test_normal = np.argmax(labels_test , axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "AccScore = accuracy_score(labels_predict, labels_test_normal)\n",
    "print('Accuracy Score is : ', AccScore)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "ax= plt.subplot()\n",
    "cm=confusion_matrix(labels_test_normal, labels_predict)\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);\n",
    "\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');\n",
    "ax.xaxis.set_ticklabels(['True', 'False']); ax.yaxis.set_ticklabels(['NonViolence', 'Violence']);\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ClassificationReport = classification_report(labels_test_normal,labels_predict)\n",
    "print('Classification Report is : \\n', ClassificationReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore\n",
    "\n",
    "def predict_frames(video_file_path, output_file_path, SEQUENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    Predict frames in a video and save them in a file. This function is called by the predict_frames function of the Cairo class.\n",
    "    \n",
    "    Args:\n",
    "        video_file_path: Path to the video file to be predicted\n",
    "        output_file_path: Path to the output video\n",
    "        SEQUENCE_LENGTH: Length of the sequence that will be\n",
    "    \"\"\"\n",
    "\n",
    "    # Read from the video file.\n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "    # Get the width and height of the video.\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # VideoWriter to store the output video in the disk.\n",
    "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),\n",
    "                                    video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
    "\n",
    "    # Declare a queue to store video frames.\n",
    "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
    "\n",
    "    # Store the predicted class in the video.\n",
    "    predicted_class_name = ''\n",
    "\n",
    "    # Initialize counters for violence and non-violence\n",
    "    violence_count = 0\n",
    "    non_violence_count = 0\n",
    "\n",
    "    # Iterate until the video is accessed successfully.\n",
    "    # This method reads the next video frame from the video_reader.\n",
    "    while video_reader.isOpened():\n",
    "\n",
    "        ok, frame = video_reader.read()\n",
    "\n",
    "        # If the user is not ok break the loop.\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        # Resize the Frame to fixed Dimensions.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        # Normalize the resized frame\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        # Appending the pre-processed frame into the frames list.\n",
    "        frames_queue.append(normalized_frame)\n",
    "\n",
    "        # We Need at Least number of SEQUENCE_LENGTH Frames to perform a prediction.\n",
    "        # Check if the number of frames in the queue are equal to the fixed sequence length.\n",
    "        # Predicts the class probabilities from the given frames queue.\n",
    "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
    "\n",
    "            # Pass the normalized frames to the model and get the predicted probabilities.\n",
    "            predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n",
    "\n",
    "            # Get the index of class with highest probability.\n",
    "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "\n",
    "            # Get the class name using the retrieved index.\n",
    "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "\n",
    "        # Write predicted class name on top of the frame.\n",
    "        # This function is called by the Fore.\n",
    "        if predicted_class_name == \"Violence\":\n",
    "            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n",
    "            print(Fore.RED + predicted_class_name)\n",
    "            violence_count += 1\n",
    "        else:\n",
    "            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 12)\n",
    "            print(Fore.GREEN + predicted_class_name)\n",
    "            non_violence_count += 1\n",
    "\n",
    "        # Write The frame into the disk using the VideoWriter\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    # Print out the violations and non violations.\n",
    "    if violence_count >= non_violence_count:\n",
    "        print(Fore.WHITE + \"This action is \" + Fore.RED + \"[Violence]\")\n",
    "    else:\n",
    "        print(Fore.WHITE + \"This action is \" + Fore.RED + \"[Non Violence]\")\n",
    "\n",
    "    video_reader.release()\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_webcam():\n",
    "    \"\"\"\n",
    "     Predict webcam and put result in text window. Args : None Tuple ( violence_count non_violence_count )\n",
    "    \"\"\"\n",
    "    # Inside here, put number 0 as to use laptop webcam and use 2 if using external webcam\n",
    "    video_reader = cv2.VideoCapture(0)\n",
    "\n",
    "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
    "\n",
    "    predicted_class_name = ''\n",
    "\n",
    "    violence_count = 0\n",
    "    non_violence_count = 0\n",
    "\n",
    "    # This function reads the video and returns the frame.\n",
    "    while True:\n",
    "        ok, frame = video_reader.read()\n",
    "\n",
    "        # If the user is not ok break the loop.\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        frames_queue.append(normalized_frame)\n",
    "\n",
    "        # This function is used to calculate the number of frames in the queue.\n",
    "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
    "            predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis=0))[0]\n",
    "\n",
    "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "\n",
    "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "\n",
    "            # This method is used to add violations and non violations to the plot.\n",
    "            if predicted_class_name == \"Violence\":\n",
    "                cv2.putText(frame, predicted_class_name, (5,100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n",
    "                print(Fore.RED + \"Violence\")\n",
    "                violence_count +=1\n",
    "            else:\n",
    "                cv2.putText(frame, predicted_class_name, (5,100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n",
    "                print(Fore.GREEN + \"Non Violence\")\n",
    "                non_violence_count +=1\n",
    "\n",
    "            cv2.rectangle(frame, (0, 0), (200,200), (0,255,0), 2)\n",
    "\n",
    "            frames_queue.clear()\n",
    "\n",
    "        cv2.imshow('Action Detection', frame)\n",
    "\n",
    "        # Wait for a key to be pressed.\n",
    "        if cv2.waitKey(1) & 0xFF ==ord('q'):\n",
    "            break\n",
    "\n",
    "    # Print out the violations and non violations.\n",
    "    if violence_count >= non_violence_count:\n",
    "        print(Fore.WHITE + \"This action is \" + Fore.RED + \" [Violence]\")\n",
    "    else:\n",
    "        print(Fore.WHITE + \"This action is \" + Fore.RED + \" [Non Violence]\")\n",
    "\n",
    "    video_reader.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "\n",
    "# To show Random Frames from the saved output predicted video (output predicted video doesn't show on the notebook but can be downloaded)\n",
    "def show_pred_frames(pred_video_path):\n",
    "    \"\"\"\n",
    "    Shows the frames that have been predicted by the predictor. This is useful for visualizing the prediction results in a way that can be visualized by clicking on the pred_video_path\n",
    "    \n",
    "    Args:\n",
    "      pred_video_path: Path to the video\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(20,15))\n",
    "\n",
    "    video_reader = cv2.VideoCapture(pred_video_path)\n",
    "\n",
    "    # Get the number of frames in the video.\n",
    "    frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Get Random Frames from the video then Sort it\n",
    "    random_range = sorted(random.sample(range (SEQUENCE_LENGTH , frames_count ), 12))\n",
    "\n",
    "    # Plot a random frame position of the video.\n",
    "    for counter, random_index in enumerate(random_range, 1):\n",
    "\n",
    "        plt.subplot(5, 4, counter)\n",
    "\n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)\n",
    "\n",
    "        ok, frame = video_reader.read()\n",
    "\n",
    "        # If the user is not ok break the loop.\n",
    "        if not ok:\n",
    "          break\n",
    "\n",
    "        frame = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.imshow(frame);ax.figure.set_size_inches(20,20);plt.tight_layout()\n",
    "\n",
    "    video_reader.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the output video path. This is a helper function to test the generation of video files for use with\n",
    "\n",
    "# Construct the output video path.\n",
    "test_videos_directory = 'test_videos'\n",
    "os.makedirs(test_videos_directory, exist_ok = True)\n",
    "\n",
    "output_video_file_path = f'{test_videos_directory}/Output-Test-Video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_file_path = \"real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_41.mp4\"\n",
    "\n",
    "# Perform Prediction on the Test Video.\n",
    "predict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n",
    "\n",
    "# Show random frames from the output video\n",
    "# show_pred_frames(output_video_file_path)\n",
    "\n",
    "# Play the actual video\n",
    "Video_Play(input_video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mThis action is \u001b[31m [Violence]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@370.048] global cap_v4l.cpp:997 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@370.048] global obsensor_uvc_stream_channel.cpp:159 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "predict_webcam()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
