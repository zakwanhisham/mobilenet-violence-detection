{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install tensorflow\n",
    "pip install keras\n",
    "pip install opencv-python\n",
    "pip install scikit-learn\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow\n",
    "import keras\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use(\"seaborn\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Video_Play(filepath):\n",
    "    cap = cv2.VideoCapture(filepath)\n",
    "\n",
    "    if (cap.isOpened == False):\n",
    "        print(\"Error opening video file\")\n",
    "    \n",
    "    while (cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            cv2.imshow('Frame', frame)\n",
    "\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes Directories\n",
    "NonViolnceVideos_Dir = \"real-life-violence-situations-dataset/Real Life Violence Dataset/NonViolence/\"\n",
    "ViolnceVideos_Dir = \"real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/\"\n",
    "\n",
    "# Retrieve the list of all the video files present in the Class Directory.\n",
    "NonViolence_files_names_list = os.listdir(NonViolnceVideos_Dir)\n",
    "Violence_files_names_list = os.listdir(ViolnceVideos_Dir)\n",
    "\n",
    "# Randomly select a video file from the Classes Directory.\n",
    "Random_NonViolence_Video = random.choice(NonViolence_files_names_list)\n",
    "Random_Violence_Video = random.choice(Violence_files_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video_Play(f\"{NonViolnceVideos_Dir}/{Random_NonViolence_Video}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64,64\n",
    "SEQUENCE_LENGTH = 16\n",
    "\n",
    "DATASET_DIR = \"real-life-violence-situations-dataset/Real Life Violence Dataset/\"\n",
    "CLASSES_LIST = [\"NonViolence\", \"Violence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "\n",
    "    frames_list = []\n",
    "\n",
    "    # Read the Video File\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get the total number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Calculate the the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    # Iterate through the Video Frames.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "\n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "\n",
    "        # Reading the frame from the video.\n",
    "        success, frame = video_reader.read()\n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Resize the Frame to fixed height and width.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        # Normalize the resized frame\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        # Append the normalized frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    "\n",
    "\n",
    "    video_reader.release()\n",
    "\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "\n",
    "    # Iterating through all the classes.\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "\n",
    "        # Get the list of video files present in the specific class name directory.\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n",
    "\n",
    "        # Iterate through all the files present in the files list.\n",
    "        for file_name in files_list:\n",
    "\n",
    "            # Get the complete video path.\n",
    "            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n",
    "\n",
    "            # Extract the frames of the video file.\n",
    "            frames = frames_extraction(video_file_path)\n",
    "\n",
    "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified.\n",
    "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "\n",
    "                # Append the data to their repective lists.\n",
    "                features.append(frames)\n",
    "                labels.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "\n",
    "    features = np.asarray(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return features, labels, video_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, video_file_paths = create_dataset()\n",
    "\n",
    "np.save(\"features.npy\", features)\n",
    "np.save(\"labels.npy\", labels)\n",
    "np.save(\"video_files_paths.npy\", video_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, video_files_paths = np.load(\"features.npy\") , np.load(\"labels.npy\") , np.load(\"video_files_paths.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels into one-hot-encoded vectors\n",
    "one_hot_encoded_labels = to_categorical(labels)\n",
    "\n",
    "# Split the Data into Train ( 90% ) and Test Set ( 10% ).\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.1,\n",
    "                                                                            shuffle = True, random_state = 42)\n",
    "\n",
    "print(features_train.shape,labels_train.shape )\n",
    "print(features_test.shape, labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "mobilenet = MobileNetV2( include_top=False , weights=\"imagenet\")\n",
    "\n",
    "#Fine-Tuning to make the last 40 layer trainable\n",
    "mobilenet.trainable=True\n",
    "\n",
    "for layer in mobilenet.layers[:-40]:\n",
    "  layer.trainable=False\n",
    "\n",
    "mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "    #Specifying Input to match features shape\n",
    "    model.add(Input(shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "\n",
    "    # Passing mobilenet in the TimeDistributed layer to handle the sequence\n",
    "    model.add(TimeDistributed(mobilenet))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "    lstm_fw = LSTM(units=32)\n",
    "    lstm_bw = LSTM(units=32, go_backwards = True)\n",
    "\n",
    "    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
    "\n",
    "    ########################################################################################################################\n",
    "    early_stopping_callback = EarlyStopping(monitor= 'val_accuracy', patience = 10, restore_best_weights=True)\n",
    "    reduce_lr = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=5,\n",
    "                                                             min_lr=0.00005, verbose=1)\n",
    "\n",
    "\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'sgd', metrics=[\"accuracy\"])\n",
    "\n",
    "    model.fit(x=features_train, y=labels_train, epochs = 50, batch_size=8, shuffle=True, validation_split=0.2, callbacks=[early_stopping_callback, reduce_lr])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    model_json = model.to_json()\n",
    "    with open(model_name + \".json\", \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "    # serialize weights to HDF5\n",
    "    model.save_weights(model_name + \".h5\")\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "def load_model(model_name):\n",
    "    with open(model_name + \".json\", \"r\") as json_file:\n",
    "        loaded_model_json = json_file.read()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the Model\n",
    "MoBiLSTM_model = create_model()\n",
    "\n",
    "# Plot the structure of the contructed LRCN model.\n",
    "# plot_model(MoBiLSTM_model, to_file = 'MobBiLSTM_model_structure_plot.png', show_shapes = True, show_layer_names = True)\n",
    "\n",
    "save_model(MoBiLSTM_model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoBiLSTM_model = load_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_predict = MoBiLSTM_model.predict(features_test)\n",
    "\n",
    "# Decoding the data to use in Metrics\n",
    "labels_predict = np.argmax(labels_predict , axis=1)\n",
    "labels_test_normal = np.argmax(labels_test , axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "AccScore = accuracy_score(labels_predict, labels_test_normal)\n",
    "print('Accuracy Score is : ', AccScore)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "ax= plt.subplot()\n",
    "cm=confusion_matrix(labels_test_normal, labels_predict)\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);\n",
    "\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels');\n",
    "ax.set_title('Confusion Matrix');\n",
    "ax.xaxis.set_ticklabels(['True', 'False']); ax.yaxis.set_ticklabels(['NonViolence', 'Violence']);\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "ClassificationReport = classification_report(labels_test_normal,labels_predict)\n",
    "print('Classification Report is : \\n', ClassificationReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore\n",
    "\n",
    "def predict_frames(video_file_path, output_file_path, SEQUENCE_LENGTH):\n",
    "\n",
    "    # Read from the video file.\n",
    "    video_reader = cv2.VideoCapture(video_file_path)\n",
    "\n",
    "    # Get the width and height of the video.\n",
    "    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # VideoWriter to store the output video in the disk.\n",
    "    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc('m', 'p', '4', 'v'),\n",
    "                                    video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n",
    "\n",
    "    # Declare a queue to store video frames.\n",
    "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
    "\n",
    "    # Store the predicted class in the video.\n",
    "    predicted_class_name = ''\n",
    "\n",
    "    # Initialize counters for violence and non-violence\n",
    "    violence_count = 0\n",
    "    non_violence_count = 0\n",
    "\n",
    "    # Iterate until the video is accessed successfully.\n",
    "    while video_reader.isOpened():\n",
    "\n",
    "        ok, frame = video_reader.read()\n",
    "\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        # Resize the Frame to fixed Dimensions.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        # Normalize the resized frame\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        # Appending the pre-processed frame into the frames list.\n",
    "        frames_queue.append(normalized_frame)\n",
    "\n",
    "        # We Need at Least number of SEQUENCE_LENGTH Frames to perform a prediction.\n",
    "        # Check if the number of frames in the queue are equal to the fixed sequence length.\n",
    "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
    "\n",
    "            # Pass the normalized frames to the model and get the predicted probabilities.\n",
    "            predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n",
    "\n",
    "            # Get the index of class with highest probability.\n",
    "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "\n",
    "            # Get the class name using the retrieved index.\n",
    "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "\n",
    "        # Write predicted class name on top of the frame.\n",
    "        if predicted_class_name == \"Violence\":\n",
    "            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n",
    "            print(Fore.RED + predicted_class_name)\n",
    "            violence_count += 1\n",
    "        else:\n",
    "            cv2.putText(frame, predicted_class_name, (5, 100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 12)\n",
    "            print(Fore.GREEN + predicted_class_name)\n",
    "            non_violence_count += 1\n",
    "\n",
    "        # Write The frame into the disk using the VideoWriter\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    if violence_count >= non_violence_count:\n",
    "        print(Fore.WHITE + \"This action is \" + Fore.RED + \"[Violence]\")\n",
    "    else:\n",
    "        print(Fore.WHITE + \"This action is \" + Fore.RED + \"[Non Violence]\")\n",
    "\n",
    "    video_reader.release()\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_webcam():\n",
    "    video_reader = cv2.VideoCapture(2)\n",
    "\n",
    "    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n",
    "\n",
    "    predicted_class_name = ''\n",
    "\n",
    "    violence_count = 0\n",
    "    non_violence_count = 0\n",
    "\n",
    "    while True:\n",
    "        ok, frame = video_reader.read()\n",
    "\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        frames_queue.append(normalized_frame)\n",
    "\n",
    "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
    "            predicted_labels_probabilities = MoBiLSTM_model.predict(np.expand_dims(frames_queue, axis=0))[0]\n",
    "\n",
    "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "\n",
    "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "\n",
    "            if predicted_class_name == \"Violence\":\n",
    "                cv2.putText(frame, predicted_class_name, (5,100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n",
    "                print(Fore.RED + \"Violence\")\n",
    "                violence_count +=1\n",
    "            else:\n",
    "                cv2.putText(frame, predicted_class_name, (5,100), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), 12)\n",
    "                print(Fore.GREEN + \"Non Violence\")\n",
    "                non_violence_count +=1\n",
    "\n",
    "            cv2.rectangle(frame, (0, 0), (200,200), (0,255,0), 2)\n",
    "\n",
    "            frames_queue.clear()\n",
    "\n",
    "        cv2.imshow('Action Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF ==ord('q'):\n",
    "            break\n",
    "\n",
    "    if violence_count >= non_violence_count:\n",
    "        print(Fore.WHITE + \"This action is \" + Fore.RED + \" [Violence]\")\n",
    "    else:\n",
    "        print(Fore.WHITE + \"This action is \" + Fore.RED + \" [Non Violence]\")\n",
    "\n",
    "    video_reader.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "\n",
    "# To show Random Frames from the saved output predicted video (output predicted video doesn't show on the notebook but can be downloaded)\n",
    "def show_pred_frames(pred_video_path):\n",
    "\n",
    "    plt.figure(figsize=(20,15))\n",
    "\n",
    "    video_reader = cv2.VideoCapture(pred_video_path)\n",
    "\n",
    "    # Get the number of frames in the video.\n",
    "    frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Get Random Frames from the video then Sort it\n",
    "    random_range = sorted(random.sample(range (SEQUENCE_LENGTH , frames_count ), 12))\n",
    "\n",
    "    for counter, random_index in enumerate(random_range, 1):\n",
    "\n",
    "        plt.subplot(5, 4, counter)\n",
    "\n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, random_index)\n",
    "\n",
    "        ok, frame = video_reader.read()\n",
    "\n",
    "        if not ok:\n",
    "          break\n",
    "\n",
    "        frame = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        plt.imshow(frame);ax.figure.set_size_inches(20,20);plt.tight_layout()\n",
    "\n",
    "    video_reader.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the output video path.\n",
    "test_videos_directory = 'test_videos'\n",
    "os.makedirs(test_videos_directory, exist_ok = True)\n",
    "\n",
    "output_video_file_path = f'{test_videos_directory}/Output-Test-Video.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "\u001b[32m\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNonViolence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNonViolence\n",
      "\u001b[37mThis action is\u001b[31m[Violence]\n"
     ]
    }
   ],
   "source": [
    "input_video_file_path = \"real-life-violence-situations-dataset/Real Life Violence Dataset/Violence/V_41.mp4\"\n",
    "# input_video_file_path = \"/content/uccrime_Shooting039_x264.mp4\"\n",
    "\n",
    "# Perform Prediction on the Test Video.\n",
    "predict_frames(input_video_file_path, output_video_file_path, SEQUENCE_LENGTH)\n",
    "\n",
    "# Show random frames from the output video\n",
    "# show_pred_frames(output_video_file_path)\n",
    "\n",
    "# Play the actual video\n",
    "Video_Play(input_video_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[31mViolence\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "\u001b[32mNon Violence\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "\u001b[32mNon Violence\n"
     ]
    }
   ],
   "source": [
    "predict_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
